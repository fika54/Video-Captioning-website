import whisper
import os

"""
Multiple Pretrained models that tradeoff Accuracy for speed.

Model Name	Parameters	Relative Size	 Accuracy	 Speed
tiny	    ~39M	    游릭 Smallest	    游댮 Low	    游릭 Fastest
base	    ~74M	    游릭 Small	    游 Low-med	游릭 Fast
small	    ~244M	    游리 Medium	    游리 Decent	游리 Medium
medium	    ~769M	    游 Large	    游릭 High	    游댮 Slower
large	    ~1550M	    游댮 Largest	    游릭 Best	    游댮 Slowest
large-v2	~1550M	    游댮 Largest	    游릭 Best	    游댮 Slowest
large-v3	~1550M	    游댯 Most recent	游릭 Best++	游댮 Slowest
"""
model = whisper.load_model("medium")

#transcribes the audio and segments it
def transcribe_audio(audio_path, mode="batch"):
    result = model.transcribe(audio_path, word_timestamps=(True)) #always transcribes word by word

    #splits the transcription into individual words and their time stamps
    words = []
    for seg in result.get("segments", []):
        words.extend(seg.get("words", []))

    #segments the words into chunks based either on pauses or the length of a given sentence
    segments = []
    istart = 0
    i = 1
    step = 0
    while i < len(words):
        diff = words[i]['start'] - words[i-1]['end']
        segdiff = i - istart

        if diff > 0.2 or segdiff > 5:
            chunk = words[istart:i]
            istart = i
            if not chunk:
                continue

            segment = {
                "start": chunk[0]["start"],
                "end": chunk[-1]["end"],
                "text": " ".join([w["word"] for w in chunk]),
                "words": chunk
            }
            segments.append(segment)

        i = i+1

    #checks the last segment in the transcript
    chunk = words[istart:i]
    istart = i

    if chunk:
        segment = {
            "start": chunk[0]["start"],
            "end": chunk[-1]["end"],
            "text": " ".join([w["word"] for w in chunk]),
            "words": chunk
        }
        segments.append(segment)

    os.remove(audio_path)

    return segments